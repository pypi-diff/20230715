# Comparing `tmp/autonomi_nos-0.0.6a1-py3-none-any.whl.zip` & `tmp/autonomi_nos-0.0.7a2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,66 +1,68 @@
-Zip file size: 76103 bytes, number of entries: 64
--rw-rw-r--  2.0 unx      164 b- defN 23-Jul-06 04:43 nos/__init__.py
--rw-rw-r--  2.0 unx      561 b- defN 23-Jul-06 04:43 nos/constants.py
--rw-rw-r--  2.0 unx       93 b- defN 23-Jul-06 04:43 nos/exceptions.py
--rw-rw-r--  2.0 unx      940 b- defN 23-Jul-06 04:43 nos/logging.py
--rw-rw-r--  2.0 unx     2778 b- defN 23-Jul-06 04:43 nos/protoc.py
--rw-rw-r--  2.0 unx       24 b- defN 23-Jul-06 04:43 nos/version.py
--rw-rw-r--  2.0 unx      369 b- defN 23-Jul-06 04:43 nos/cli/cli.py
--rw-rw-r--  2.0 unx     4230 b- defN 23-Jul-06 04:43 nos/cli/docker.py
--rw-rw-r--  2.0 unx     1294 b- defN 23-Jul-06 04:43 nos/cli/hub.py
--rw-rw-r--  2.0 unx     7543 b- defN 23-Jul-06 04:43 nos/cli/predict.py
--rw-rw-r--  2.0 unx     5962 b- defN 23-Jul-06 04:43 nos/cli/serve_http.py
--rw-rw-r--  2.0 unx     1713 b- defN 23-Jul-06 04:43 nos/cli/system.py
--rw-rw-r--  2.0 unx      425 b- defN 23-Jul-06 04:43 nos/cli/utils.py
--rw-rw-r--  2.0 unx      396 b- defN 23-Jul-06 04:43 nos/client/__init__.py
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-06 04:43 nos/client/exceptions.py
--rw-rw-r--  2.0 unx    12378 b- defN 23-Jul-06 04:43 nos/client/grpc.py
--rw-rw-r--  2.0 unx     1285 b- defN 23-Jul-06 04:43 nos/common/__init__.py
--rw-rw-r--  2.0 unx      204 b- defN 23-Jul-06 04:43 nos/common/cloudpickle.py
--rw-rw-r--  2.0 unx     2712 b- defN 23-Jul-06 04:43 nos/common/profiler.py
--rw-rw-r--  2.0 unx     9446 b- defN 23-Jul-06 04:43 nos/common/spec.py
--rw-rw-r--  2.0 unx     6076 b- defN 23-Jul-06 04:43 nos/common/system.py
--rw-rw-r--  2.0 unx      518 b- defN 23-Jul-06 04:43 nos/common/tasks.py
--rw-rw-r--  2.0 unx     5984 b- defN 23-Jul-06 04:43 nos/common/types.py
--rw-rw-r--  2.0 unx      942 b- defN 23-Jul-06 04:43 nos/common/io/__init__.py
--rw-rw-r--  2.0 unx     1050 b- defN 23-Jul-06 04:43 nos/common/io/video/base.py
--rw-rw-r--  2.0 unx     8424 b- defN 23-Jul-06 04:43 nos/common/io/video/opencv.py
--rw-rw-r--  2.0 unx     5832 b- defN 23-Jul-06 04:43 nos/compilers/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-06 04:43 nos/compilers/trt/__init__.py
--rw-rw-r--  2.0 unx     2739 b- defN 23-Jul-06 04:43 nos/compilers/trt/ops/group_norm.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jul-06 04:43 nos/executors/__init__.py
--rw-rw-r--  2.0 unx     7544 b- defN 23-Jul-06 04:43 nos/executors/ray.py
--rw-rw-r--  2.0 unx     3531 b- defN 23-Jul-06 04:43 nos/hub/__init__.py
--rw-rw-r--  2.0 unx     2195 b- defN 23-Jul-06 04:43 nos/hub/config.py
--rw-rw-r--  2.0 unx       59 b- defN 23-Jul-06 04:43 nos/managers/__init__.py
--rw-rw-r--  2.0 unx     7911 b- defN 23-Jul-06 04:43 nos/managers/model.py
--rw-rw-r--  2.0 unx      301 b- defN 23-Jul-06 04:43 nos/models/__init__.py
--rw-rw-r--  2.0 unx     9931 b- defN 23-Jul-06 04:43 nos/models/clip.py
--rw-rw-r--  2.0 unx     3285 b- defN 23-Jul-06 04:43 nos/models/faster_rcnn.py
--rw-rw-r--  2.0 unx     1507 b- defN 23-Jul-06 04:43 nos/models/sam.py
--rw-rw-r--  2.0 unx    16095 b- defN 23-Jul-06 04:43 nos/models/stable_diffusion.py
--rw-rw-r--  2.0 unx    10128 b- defN 23-Jul-06 04:43 nos/models/yolox.py
--rw-rw-r--  2.0 unx       63 b- defN 23-Jul-06 04:43 nos/models/openmmlab/__init__.py
--rw-rw-r--  2.0 unx     3066 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/mmdetection.py
--rw-rw-r--  2.0 unx      370 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py
--rw-rw-r--  2.0 unx     3187 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py
--rw-rw-r--  2.0 unx     1765 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py
--rw-rw-r--  2.0 unx     3828 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py
--rw-rw-r--  2.0 unx      304 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py
--rw-rw-r--  2.0 unx     5340 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py
--rw-rw-r--  2.0 unx      177 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py
--rw-rw-r--  2.0 unx     2437 b- defN 23-Jul-06 04:43 nos/proto/nos_service.proto
--rw-rw-r--  2.0 unx     6193 b- defN 23-Jul-06 04:43 nos/server/__init__.py
--rw-rw-r--  2.0 unx     6556 b- defN 23-Jul-06 04:43 nos/server/docker.py
--rw-rw-r--  2.0 unx     6363 b- defN 23-Jul-06 04:43 nos/server/runtime.py
--rw-rw-r--  2.0 unx     7326 b- defN 23-Jul-06 04:43 nos/server/service.py
--rw-rw-r--  2.0 unx      373 b- defN 23-Jul-06 04:43 nos/test/benchmark.py
--rw-rw-r--  2.0 unx     4252 b- defN 23-Jul-06 04:43 nos/test/conftest.py
--rw-rw-r--  2.0 unx     2626 b- defN 23-Jul-06 04:43 nos/test/utils.py
--rw-rw-r--  2.0 unx     1068 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     6804 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       86 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        4 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     5441 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/RECORD
-64 files, 214382 bytes uncompressed, 67455 bytes compressed:  68.5%
+Zip file size: 84104 bytes, number of entries: 66
+-rw-rw-r--  2.0 unx      164 b- defN 23-Jul-12 06:39 nos/__init__.py
+-rw-rw-r--  2.0 unx      635 b- defN 23-Jul-14 21:48 nos/constants.py
+-rw-rw-r--  2.0 unx       93 b- defN 23-Jun-16 01:40 nos/exceptions.py
+-rw-rw-r--  2.0 unx      940 b- defN 23-May-09 22:15 nos/logging.py
+-rw-rw-r--  2.0 unx     2778 b- defN 23-May-19 22:25 nos/protoc.py
+-rw-rw-r--  2.0 unx       24 b- defN 23-Jul-13 18:07 nos/version.py
+-rw-rw-r--  2.0 unx    11371 b- defN 23-Jul-13 16:56 nos/cli/benchmark.py
+-rw-rw-r--  2.0 unx      540 b- defN 23-Jul-13 18:49 nos/cli/cli.py
+-rw-rw-r--  2.0 unx     4222 b- defN 23-Jul-13 19:08 nos/cli/docker.py
+-rw-rw-r--  2.0 unx     1294 b- defN 23-May-05 08:07 nos/cli/hub.py
+-rw-rw-r--  2.0 unx     7543 b- defN 23-Jun-04 17:53 nos/cli/predict.py
+-rw-rw-r--  2.0 unx     5962 b- defN 23-May-15 21:47 nos/cli/serve_http.py
+-rw-rw-r--  2.0 unx     1706 b- defN 23-Jul-13 19:08 nos/cli/system.py
+-rw-rw-r--  2.0 unx      425 b- defN 23-May-09 22:15 nos/cli/utils.py
+-rw-rw-r--  2.0 unx      396 b- defN 23-Jul-05 07:11 nos/client/__init__.py
+-rw-rw-r--  2.0 unx      351 b- defN 23-Jul-13 22:42 nos/client/exceptions.py
+-rw-rw-r--  2.0 unx    13192 b- defN 23-Jul-14 18:16 nos/client/grpc.py
+-rw-rw-r--  2.0 unx     1285 b- defN 23-Jul-13 16:56 nos/common/__init__.py
+-rw-rw-r--  2.0 unx      204 b- defN 23-May-28 04:11 nos/common/cloudpickle.py
+-rw-rw-r--  2.0 unx    10364 b- defN 23-Jul-14 21:48 nos/common/profiler.py
+-rw-rw-r--  2.0 unx     9446 b- defN 23-Jun-15 22:10 nos/common/spec.py
+-rw-rw-r--  2.0 unx     6076 b- defN 23-Jul-13 19:52 nos/common/system.py
+-rw-rw-r--  2.0 unx      593 b- defN 23-Jul-13 16:56 nos/common/tasks.py
+-rw-rw-r--  2.0 unx     5984 b- defN 23-Jun-15 22:10 nos/common/types.py
+-rw-rw-r--  2.0 unx     1499 b- defN 23-Jul-14 19:09 nos/common/io/__init__.py
+-rw-rw-r--  2.0 unx     1050 b- defN 23-Jun-15 06:14 nos/common/io/video/base.py
+-rw-rw-r--  2.0 unx     8424 b- defN 23-Jun-15 17:06 nos/common/io/video/opencv.py
+-rw-rw-r--  2.0 unx     5832 b- defN 23-Jun-30 21:21 nos/compilers/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-28 04:05 nos/compilers/trt/__init__.py
+-rw-rw-r--  2.0 unx     2739 b- defN 23-Jun-28 04:05 nos/compilers/trt/ops/group_norm.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-18 17:00 nos/executors/__init__.py
+-rw-rw-r--  2.0 unx     7544 b- defN 23-Jul-07 20:28 nos/executors/ray.py
+-rw-rw-r--  2.0 unx     3615 b- defN 23-Jul-13 18:49 nos/hub/__init__.py
+-rw-rw-r--  2.0 unx     2195 b- defN 23-Jun-01 19:04 nos/hub/config.py
+-rw-rw-r--  2.0 unx       59 b- defN 23-Jun-15 22:10 nos/managers/__init__.py
+-rw-rw-r--  2.0 unx     7911 b- defN 23-Jul-14 17:38 nos/managers/model.py
+-rw-rw-r--  2.0 unx      339 b- defN 23-Jul-13 22:53 nos/models/__init__.py
+-rw-rw-r--  2.0 unx      879 b- defN 23-Jul-14 19:29 nos/models/_noop.py
+-rw-rw-r--  2.0 unx     9931 b- defN 23-Jul-10 17:19 nos/models/clip.py
+-rw-rw-r--  2.0 unx     2834 b- defN 23-Jul-14 19:19 nos/models/faster_rcnn.py
+-rw-rw-r--  2.0 unx     1507 b- defN 23-Jul-08 01:45 nos/models/sam.py
+-rw-rw-r--  2.0 unx    16095 b- defN 23-Jul-08 01:45 nos/models/stable_diffusion.py
+-rw-rw-r--  2.0 unx    10128 b- defN 23-Jul-14 20:38 nos/models/yolox.py
+-rw-rw-r--  2.0 unx       63 b- defN 23-Jun-02 17:26 nos/models/openmmlab/__init__.py
+-rw-rw-r--  2.0 unx     3066 b- defN 23-Jun-09 19:19 nos/models/openmmlab/mmdetection/mmdetection.py
+-rw-rw-r--  2.0 unx      370 b- defN 23-May-09 22:15 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py
+-rw-rw-r--  2.0 unx     3187 b- defN 23-May-09 22:15 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py
+-rw-rw-r--  2.0 unx     1765 b- defN 23-May-09 22:15 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py
+-rw-rw-r--  2.0 unx     3828 b- defN 23-May-10 02:43 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py
+-rw-rw-r--  2.0 unx      304 b- defN 23-May-09 22:15 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py
+-rw-rw-r--  2.0 unx     5340 b- defN 23-May-09 22:15 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py
+-rw-rw-r--  2.0 unx      177 b- defN 23-May-10 02:43 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py
+-rw-rw-r--  2.0 unx     2437 b- defN 23-Jul-13 16:56 nos/proto/nos_service.proto
+-rw-rw-r--  2.0 unx    10198 b- defN 23-Jul-14 23:41 nos/server/__init__.py
+-rw-rw-r--  2.0 unx     6725 b- defN 23-Jul-13 19:27 nos/server/_docker.py
+-rw-rw-r--  2.0 unx     6482 b- defN 23-Jul-14 21:09 nos/server/_runtime.py
+-rw-rw-r--  2.0 unx     7989 b- defN 23-Jul-14 23:49 nos/server/_service.py
+-rw-rw-r--  2.0 unx      373 b- defN 23-Jul-14 17:54 nos/test/benchmark.py
+-rw-rw-r--  2.0 unx     4237 b- defN 23-Jul-13 21:24 nos/test/conftest.py
+-rw-rw-r--  2.0 unx     2626 b- defN 23-Jul-07 20:28 nos/test/utils.py
+-rw-r--r--  2.0 unx     1068 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     6800 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       87 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       12 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5602 b- defN 23-Jul-14 23:58 autonomi_nos-0.0.7a2.dist-info/RECORD
+66 files, 240997 bytes uncompressed, 75220 bytes compressed:  68.8%
```

## zipnote {}

```diff
@@ -12,14 +12,17 @@
 
 Filename: nos/protoc.py
 Comment: 
 
 Filename: nos/version.py
 Comment: 
 
+Filename: nos/cli/benchmark.py
+Comment: 
+
 Filename: nos/cli/cli.py
 Comment: 
 
 Filename: nos/cli/docker.py
 Comment: 
 
 Filename: nos/cli/hub.py
@@ -102,14 +105,17 @@
 
 Filename: nos/managers/model.py
 Comment: 
 
 Filename: nos/models/__init__.py
 Comment: 
 
+Filename: nos/models/_noop.py
+Comment: 
+
 Filename: nos/models/clip.py
 Comment: 
 
 Filename: nos/models/faster_rcnn.py
 Comment: 
 
 Filename: nos/models/sam.py
@@ -150,44 +156,44 @@
 
 Filename: nos/proto/nos_service.proto
 Comment: 
 
 Filename: nos/server/__init__.py
 Comment: 
 
-Filename: nos/server/docker.py
+Filename: nos/server/_docker.py
 Comment: 
 
-Filename: nos/server/runtime.py
+Filename: nos/server/_runtime.py
 Comment: 
 
-Filename: nos/server/service.py
+Filename: nos/server/_service.py
 Comment: 
 
 Filename: nos/test/benchmark.py
 Comment: 
 
 Filename: nos/test/conftest.py
 Comment: 
 
 Filename: nos/test/utils.py
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/LICENSE
+Filename: autonomi_nos-0.0.7a2.dist-info/LICENSE
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/METADATA
+Filename: autonomi_nos-0.0.7a2.dist-info/METADATA
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/WHEEL
+Filename: autonomi_nos-0.0.7a2.dist-info/WHEEL
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/entry_points.txt
+Filename: autonomi_nos-0.0.7a2.dist-info/entry_points.txt
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/top_level.txt
+Filename: autonomi_nos-0.0.7a2.dist-info/top_level.txt
 Comment: 
 
-Filename: autonomi_nos-0.0.6a1.dist-info/RECORD
+Filename: autonomi_nos-0.0.7a2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nos/constants.py

```diff
@@ -13,7 +13,9 @@
 NOS_CACHE_DIR.mkdir(parents=True, exist_ok=True)
 NOS_MODELS_DIR.mkdir(parents=True, exist_ok=True)
 NOS_LOG_DIR.mkdir(parents=True, exist_ok=True)
 NOS_TMP_DIR.mkdir(parents=True, exist_ok=True)
 
 DEFAULT_GRPC_PORT = 50051
 DEFAULT_HTTP_PORT = 8000
+
+NOS_PROFILING_ENABLED = bool(int(os.getenv("NOS_PROFILING_ENABLED", 0)))
```

## nos/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.0.6a1"
+__version__ = "0.0.7a2"
```

## nos/cli/cli.py

```diff
@@ -1,17 +1,29 @@
 import typer
 
+
+try:
+    import torch
+except ImportError:
+    torch = None
+
+
 from nos.cli.docker import docker_cli
 from nos.cli.hub import hub_cli
 from nos.cli.predict import predict_cli
 from nos.cli.system import system_cli
 
 
 app_cli = typer.Typer(no_args_is_help=True)
 app_cli.add_typer(hub_cli)
 app_cli.add_typer(system_cli)
 app_cli.add_typer(docker_cli)
 app_cli.add_typer(predict_cli)
 
+if torch is not None:
+    from nos.cli.benchmark import benchmark_cli
+
+    app_cli.add_typer(benchmark_cli)
+
 
 if __name__ == "__main__":
     app_cli()
```

## nos/cli/docker.py

```diff
@@ -1,13 +1,13 @@
 import rich.console
 import rich.status
 import typer
 
 import docker
-from nos.server.runtime import InferenceServiceRuntime
+from nos.server import InferenceServiceRuntime
 
 
 docker_cli = typer.Typer(name="docker", help="NOS Docker CLI.", no_args_is_help=True)
 console = rich.console.Console()
 
 
 def get_running_inference_service_runtime_container(id: str = None) -> docker.models.containers.Container:
```

## nos/cli/system.py

```diff
@@ -2,15 +2,15 @@
 from rich.console import Console
 from rich.json import JSON
 from rich.panel import Panel
 
 from docker.errors import APIError
 from nos.common.system import get_system_info, has_gpu
 from nos.logging import logger
-from nos.server.docker import DockerRuntime
+from nos.server import DockerRuntime
 
 
 system_cli = typer.Typer(name="system", help="NOS System CLI.", no_args_is_help=True)
 console = Console()
 
 
 @system_cli.command("info")
```

## nos/client/exceptions.py

```diff
@@ -1,5 +1,15 @@
 """Custom exceptions for the nos client."""
+from dataclasses import dataclass
 
 
+@dataclass(frozen=True)
 class NosClientException(Exception):
-    pass
+    """Base exception for the nos client."""
+
+    message: str
+    """Exception message."""
+    exc: Exception = None
+    """Exception object."""
+
+    def __str__(self) -> str:
+        return f"{self.message}"
```

## nos/client/grpc.py

```diff
@@ -1,19 +1,20 @@
 """gRPC client for NOS service."""
 import time
+import traceback
 from dataclasses import dataclass, field
 from functools import lru_cache
 from typing import Any, Callable, Dict, List
 
 import grpc
 from google.protobuf import empty_pb2
 
 from nos.client.exceptions import NosClientException
 from nos.common import ModelSpec, TaskType, loads
-from nos.constants import DEFAULT_GRPC_PORT
+from nos.constants import DEFAULT_GRPC_PORT, NOS_PROFILING_ENABLED
 from nos.logging import logger
 from nos.protoc import import_module
 from nos.version import __version__
 
 
 nos_service_pb2 = import_module("nos_service_pb2")
 nos_service_pb2_grpc = import_module("nos_service_pb2_grpc")
@@ -98,15 +99,15 @@
             NosClientException: If the server fails to respond to the connection request.
         """
         if not self._stub:
             self._channel = grpc.insecure_channel(self.address)
             try:
                 self._stub = nos_service_pb2_grpc.InferenceServiceStub(self._channel)
             except Exception as e:
-                raise NosClientException(f"Failed to connect to server ({e})")
+                raise NosClientException(f"Failed to connect to server ({e})", e)
         assert self._channel
         assert self._stub
         return self._stub
 
     def IsHealthy(self) -> bool:
         """Check if the gRPC server is healthy.
 
@@ -114,51 +115,50 @@
             bool: True if the server is running, False otherwise.
         Raises:
             NosClientException: If the server fails to respond to the ping.
         """
         try:
             response: nos_service_pb2.PingResponse = self.stub.Ping(empty_pb2.Empty())
             return response.status == "ok"
-        except grpc.RpcError as exc:
-            raise NosClientException(f"Failed to ping server ({exc})")
+        except grpc.RpcError as e:
+            raise NosClientException(f"Failed to ping server (details={e.details()})", e)
 
     def WaitForServer(self, timeout: int = 60, retry_interval: int = 5) -> None:
         """Ping the gRPC server for health.
 
         Args:
             timeout (int, optional): Timeout in seconds. Defaults to 60.
             retry_interval (int, optional): Retry interval in seconds. Defaults to 5.
         Returns:
             bool: True if the server is running, False otherwise.
         Raises:
             NosClientException: If the server fails to respond to the ping or times out.
         """
-        exc = None
         st = time.time()
         while time.time() - st <= timeout:
             try:
                 return self.IsHealthy()
             except Exception:
                 logger.warning("Waiting for server to start... (elapsed={:.0f}s)".format(time.time() - st))
                 time.sleep(retry_interval)
-        raise NosClientException(f"Failed to ping server ({exc})")
+        raise NosClientException("Failed to ping server.")
 
     def GetServiceVersion(self) -> str:
         """Get service version.
 
         Returns:
             str: Service version (e.g. 0.0.4).
         Raises:
             NosClientException: If the server fails to respond to the request.
         """
         try:
             response: nos_service_pb2.ServiceInfoResponse = self.stub.GetServiceInfo(empty_pb2.Empty())
             return response.version
         except grpc.RpcError as e:
-            raise NosClientException(f"Failed to get service info ({e})")
+            raise NosClientException(f"Failed to get service info (details={e.details()})", e)
 
     def CheckCompatibility(self) -> bool:
         """Check if the service version is compatible with the client.
 
         Returns:
             bool: True if the service version is compatible, False otherwise.
         Raises:
@@ -181,15 +181,15 @@
         Raises:
             NosClientException: If the server fails to respond to the request.
         """
         try:
             response: nos_service_pb2.ModelListResponse = self.stub.ListModels(empty_pb2.Empty())
             return [ModelSpec(name=minfo.name, task=TaskType(minfo.task)) for minfo in response.models]
         except grpc.RpcError as e:
-            raise NosClientException(f"Failed to list models ({e})")
+            raise NosClientException(f"Failed to list models (details={e.details()})", e)
 
     def GetModelInfo(self, spec: ModelSpec) -> ModelSpec:
         """Get the relevant model information from the model name.
 
         Note: This may be possible only after initialization, as we need to inspect the
         HW to understand the configurable image resolutions, batch sizes etc.
 
@@ -201,15 +201,15 @@
                 nos_service_pb2.ModelInfoRequest(
                     request=nos_service_pb2.ModelInfo(task=spec.task.value, name=spec.name)
                 )
             )
             model_spec: ModelSpec = loads(response.response_bytes)
             return model_spec
         except grpc.RpcError as e:
-            raise NosClientException(f"Failed to get model info ({e})")
+            raise NosClientException(f"Failed to get model info (details={(e.details())})", e)
 
     @lru_cache(maxsize=8)  # noqa: B019
     def Module(self, task: TaskType, model_name: str) -> "InferenceModule":
         """Instantiate a model module.
 
         Args:
             task (TaskType): Task used for prediction.
@@ -310,26 +310,39 @@
             Dict[str, Any]: Inference response.
         Raises:
             NosClientException: If the server fails to respond to the request.
         """
         # Check if the input dictionary is consistent
         # with inputs/outputs defined in `spec.signature`
         # and then encode it.
+        st = time.perf_counter()
         inputs = self._spec.signature._encode_inputs(inputs)
+        if NOS_PROFILING_ENABLED:
+            logger.debug(f"Encoded inputs, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms")
         request = nos_service_pb2.InferenceRequest(
             model=nos_service_pb2.ModelInfo(
                 task=self.task.value,
                 name=self.model_name,
             ),
             inputs=inputs,
         )
         try:
-
+            mid = time.perf_counter()
             response = self.stub.Run(request)
+            if NOS_PROFILING_ENABLED:
+                logger.debug(f"Executed request, elapsed={(time.perf_counter() - mid) * 1e3:.1f}ms")
+
+            mid = time.perf_counter()
             response = loads(response.response_bytes)
+            if NOS_PROFILING_ENABLED:
+                logger.debug(f"Decoded response, elapsed={(time.perf_counter() - mid) * 1e3:.1f}ms")
             return response
-        except Exception as e:
-            import traceback
-
-            raise NosClientException(
-                f"""Failed to run model {self.model_name} ({e})""" f"""\nTraceback\n""" f"""{traceback.format_exc()}"""
+        except grpc.RpcError as e:
+            logger.error(
+                f"""Run() failed"""
+                f"""\nrequest={request}"""
+                f"""\ninputs={inputs}"""
+                f"""\nerror: {e.details()}"""
+                f"""\n\nTraceback"""
+                f"""\n{traceback.format_exc()}"""
             )
+            raise NosClientException(f"Failed to run model {self.model_name} (details={e.details()})", e)
```

## nos/common/profiler.py

```diff
@@ -1,45 +1,260 @@
 import contextlib
 import datetime
+import gc
+import json
+import os
+import time
+from dataclasses import dataclass, field
 from pathlib import Path
-from typing import Optional
+from typing import Any, Dict, List, Union
 
 import pandas as pd
+import psutil
 import torch
 from torch.profiler import ProfilerActivity, profile  # noqa
 from torch.profiler import record_function as _record_function  # noqa
 
-from nos.common.system import has_gpu
+from nos.common import tqdm
+from nos.common.system import get_system_info, has_gpu
 from nos.constants import NOS_CACHE_DIR
 from nos.logging import logger
+from nos.version import __version__
 
 
 NOS_PROFILE_DIR = NOS_CACHE_DIR / "profile"
 NOS_PROFILE_DIR.mkdir(parents=True, exist_ok=True)
 DEFAULT_PROFILER_SCHEDULE = torch.profiler.schedule(wait=10, warmup=10, active=80, repeat=0)
 
 
-@contextlib.contextmanager
-def record_function(name: str, args: Optional[str] = None):
-    """Default NOS record_function."""
-
-    with _record_function(name, args) as rf:
-        rf._gpu_memory_usage = {}
-        rf._gpu_memory_usage[f"{name}::_before"] = torch.cuda.mem_get_info()
-        yield rf
-        rf._gpu_memory_usage[f"{name}::_after"] = torch.cuda.mem_get_info()
+@dataclass
+class ExecutionStats:
+    """Execution statistics."""
+
+    num_iterations: int
+    """Number of iterations."""
+    total_ms: float
+    """Total time in milliseconds."""
+    cpu_utilization: float
+    """CPU utilization."""
+    gpu_utilization: Union[None, float]
+    """GPU utilization."""
+
+    @property
+    def fps(self):
+        return self.num_iterations / (self.total_ms * 1e3)
+
+
+class profile_execution:
+    """Context manager for profiling execution."""
+
+    def __init__(self, name: str, iterations: int = None, duration: float = None):
+        if iterations is None and duration is None:
+            raise ValueError("Either `iterations` or `duration` must be specified.")
+        self.iterations = iterations
+        self.duration = duration
+        self.name = f"{name}::execution"
+        self.iterator = None
+        self.execution_stats = None
+
+    def __repr__(self) -> str:
+        return (
+            f"""{self.__class__.__name__} """
+            f"""(name={self.name}, iterations={self.iterations}, duration={self.duration}, """
+            f"""stats={self.execution_stats})"""
+        )
+
+    def __enter__(self) -> "profile_execution":
+        """Start profiling execution."""
+        # Note (spillai): The first call to `cpu_percent` with `interval=None` starts
+        # capturing the CPU utilization. The second call in `__exit__` returns
+        # the average CPU utilization over the duration of the context manager.
+        _ = psutil.cpu_percent(interval=None)
+        self.iterator = (
+            tqdm(duration=self.duration, desc=self.name)
+            if self.duration
+            else tqdm(total=self.iterations, desc=self.name)
+        )
+        return self
+
+    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
+        """Stop profiling."""
+        end_t = time.time()
+        cpu_util = psutil.cpu_percent(interval=None)
+        try:
+            # TOFIX (spillai): This will be fixed with torch 2.1
+            gpu_util = torch.cuda.utilization(int(os.getenv("CUDA_VISIBLE_DEVICES", None)))
+        except Exception:
+            gpu_util = None
+        self.execution_stats = ExecutionStats(
+            self.iterator.n, (end_t - self.iterator.start_t) * 1e3, cpu_util, gpu_util
+        )
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        return next(self.iterator)
+
+
+class profile_memory:
+    """Context manager for profiling memory usage (GPU/CPU)."""
+
+    def __init__(self, name: str):
+        self.name = name
+        self.memory_stats = {}
+
+    def __repr__(self) -> str:
+        return f"""{self.__class__.__name__} (name={self.name}, stats={self.memory_stats})"""
+
+    def __enter__(self) -> "profile_memory":
+        """Start profiling GPU memory usage."""
+        try:
+            free, total = torch.cuda.mem_get_info()
+            self.memory_stats[f"{self.name}::memory_gpu::pre"] = total - free
+        except Exception:
+            self.memory_stats[f"{self.name}::memory_gpu::pre"] = None
+        finally:
+            free, total = psutil.virtual_memory().available, psutil.virtual_memory().total
+            self.memory_stats[f"{self.name}::memory_cpu::pre"] = total - free
+        return self
+
+    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
+        """Stop profiling GPU memory usage."""
+        try:
+            free, total = torch.cuda.mem_get_info()
+            self.memory_stats[f"{self.name}::memory_gpu::post"] = total - free
+        except Exception:
+            self.memory_stats[f"{self.name}::memory_gpu::post"] = None
+        finally:
+            free, total = psutil.virtual_memory().available, psutil.virtual_memory().total
+            self.memory_stats[f"{self.name}::memory_cpu::post"] = total - free
+        return
+
+    def memory_usage(self):
+        return self.memory_stats
+
+
+class profiler_record:
+    """Profile record for capturing profiling data (execution profile, memory profile, etc.)."""
+
+    def __init__(self, namespace: str, **kwargs):
+        self.namespace = namespace
+        self.kwargs = kwargs
+        self.profile_data = {}
+
+    @contextlib.contextmanager
+    def profile_execution(self, name: str = None, iterations: int = None, duration: float = None) -> profile_execution:
+        """Context manager for profiling execution time."""
+        with profile_execution(f"{name}", iterations=iterations, duration=duration) as prof:
+            yield prof
+        self.profile_data[prof.name] = prof.execution_stats.__dict__
+        print(prof)
+
+    @contextlib.contextmanager
+    def profile_memory(self, name: str = None) -> profile_memory:
+        """Context manager for profiling memory usage."""
+        with profile_memory(f"{name}") as prof:
+            yield prof
+        # TODO (spillai): This is to avoid nested namespaces in the profile data dict.
+        self.profile_data.update(prof.memory_usage())
+        print(prof)
+
+    def as_dict(self) -> Dict[str, Any]:
+        """Return a dictionary representation of the profiler record."""
+        return {
+            "namespace": self.namespace,
+            "profile_data": self.profile_data,
+            **self.kwargs,
+        }
+
+
+@dataclass
+class Profiler:
+    """NOS profiler as a context manager."""
+
+    records: List[profiler_record] = field(default_factory=list)
+    """List of profiler records."""
+
+    def __enter__(self) -> "Profiler":
+        """Start profiling benchmarks, clearing all torch.cuda cache/stats ."""
+        try:
+            torch.cuda.empty_cache()
+            torch.cuda.reset_peak_memory_stats()
+            torch.cuda.reset_accumulated_memory_stats()
+        except Exception:
+            pass
+        return self
+
+    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
+        """Stop profiling benchmarks."""
+        try:
+            torch.cuda.empty_cache()
+            torch.cuda.reset_peak_memory_stats()
+            torch.cuda.reset_accumulated_memory_stats()
+        except Exception:
+            pass
+        gc.collect()
+        return
+
+    def add(self, name: str, **kwargs) -> profiler_record:
+        """Add a profiler record."""
+        if len(self.records) > 0 and self.records[0].kwargs.keys() != kwargs.keys():
+            raise ValueError("Adding a new record with different kwargs is not supported.")
+        self.records.append(profiler_record(name, **kwargs))
+        return self.records[-1]
+
+    def as_dict(self) -> Dict[str, Any]:
+        """Return a dictionary representation of the profiler."""
+        return {
+            "date": datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
+            "nos_version": __version__,
+            "sysinfo": get_system_info(docker=True, gpu=True),
+            "records": [r.as_dict() for r in self.records],
+        }
+
+    def __repr__(self) -> str:
+        """Return a string representation of the profiler."""
+        return json.dumps(self.as_dict(), indent=4)
+
+    def save(self, filename: Union[Path, str]) -> None:
+        """Save profiled results to a file."""
+        with open(str(filename), "w") as f:
+            json.dump(self.as_dict(), f, indent=4)
+
+    @classmethod
+    def load(cls, filename: Union[Path, str]) -> Dict[str, Any]:
+        """Load profiled results."""
+        with open(str(filename), "r") as f:
+            return json.load(f)
+
+    @classmethod
+    def load_metadata(cls, filename: Union[Path, str]) -> pd.DataFrame:
+        """Load profiled metadata."""
+        data = cls.load(filename)
+        _ = data.pop("records")
+        return data
+
+    @classmethod
+    def load_records(cls, filename: Union[Path, str]) -> pd.DataFrame:
+        """Load profiled records as a dataframe."""
+        data = cls.load(filename)
+        records = data.pop("records")
+        return pd.json_normalize(records)
 
 
 @contextlib.contextmanager
-def profiler(
+def _profiler(
     schedule: torch.profiler.schedule = None, profile_memory: bool = False, export_chrome_trace: bool = False
 ):
     """Default NOS profiler as a context manager."""
     activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA] if has_gpu() else [ProfilerActivity.CPU]
     with profile(activities=activities, schedule=schedule, profile_memory=profile_memory) as prof:
+        prof._sys_info = get_system_info()
+        prof._mem_prof = {}
         yield prof
 
     if export_chrome_trace:
         profile_path = Path(NOS_PROFILE_DIR) / f"chrome_trace_{datetime.datetime.utcnow().isoformat()}.json"
         prof.export_chrome_trace(str(profile_path))
         logger.debug(f"Profile saved to {profile_path}")
```

## nos/common/tasks.py

```diff
@@ -14,7 +14,10 @@
     IMAGE_GENERATION = "image_generation"
     """Image generation."""
 
     IMAGE_EMBEDDING = "image_embedding"
     """Image embedding."""
     TEXT_EMBEDDING = "text_embedding"
     """Text embedding."""
+
+    CUSTOM = "custom"
+    """Noop GPRC call for benchrmarking purposes"""
```

## nos/common/io/__init__.py

```diff
@@ -5,25 +5,36 @@
 
 from .video.opencv import VideoReader, VideoWriter  # noqa: F401
 
 
 def prepare_images(images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]) -> List[np.ndarray]:
     """Prepare images for inference.
 
+    All images are converted to np.ndarray, either as a stacked batch or a list.
+    Currently, this implementation does not stack images into a batch unless
+    they are already stacked as a batch.
+
     Args:
         images: A single image or a list of images (PIL.Image or np.ndarray)
 
     Returns:
         images: A list of images (np.ndarray)
     """
     if isinstance(images, np.ndarray):
-        images = [images]
+        # Only convert unary images as list, otherwise assume batched ndarray
+        if images.ndim < 3 or images.ndim > 4:
+            raise ValueError(f"Invalid number of dimensions for images: {images.ndim}")
+        if images.ndim == 3:
+            images = [images]
     elif isinstance(images, Image.Image):
         images = [np.asarray(images.convert("RGB"))]
     elif isinstance(images, list):
         if isinstance(images[0], Image.Image):
             images = [np.asarray(image.convert("RGB")) for image in images]
         elif isinstance(images[0], np.ndarray):
             pass
         else:
-            raise ValueError(f"Invalid type for images: {type(images[0])}")
+            raise TypeError(f"Invalid type for images: {type(images[0])}")
+    else:
+        raise TypeError(f"Invalid type for images: {type(images)}")
+    # import pdb; pdb.set_trace()
     return images
```

## nos/hub/__init__.py

```diff
@@ -88,16 +88,18 @@
                 outputs=kwargs.pop("outputs", {}),
                 func_or_cls=func_or_cls,
                 init_args=kwargs.pop("init_args", ()),
                 init_kwargs=kwargs.pop("init_kwargs", {}),
                 method_name=kwargs.pop("method_name", None),
             ),
         )
-        cls.get()._registry[ModelSpec.get_id(spec.name, spec.task)] = spec
-        logger.debug(f"Registered model: [name={model_name}]")
+        model_id = ModelSpec.get_id(spec.name, spec.task)
+        if model_id not in cls.get()._registry:
+            cls.get()._registry[model_id] = spec
+            logger.debug(f"Registered model: [name={model_name}]")
 
 
 # Alias methods
 list = Hub.list
 load = Hub.load
 register = Hub.register
 load_spec = Hub.load_spec
```

## nos/managers/model.py

```diff
@@ -62,15 +62,15 @@
             Union[ray.remote, ray.actor.ActorHandle]: Ray actor handle.
         """
         # TODO (spillai): Use the auto-tuned model spec to instantiate an
         # actor the desired memory requirements. Fractional GPU amounts
         # will need to be calculated from the target HW and model spec
         # (i.e. 0.5 on A100 vs. T4 are different).
         model_cls = spec.signature.func_or_cls
-        actor_options = {"num_gpus": 0.5 if torch.cuda.is_available() else 0}
+        actor_options = {"num_gpus": 0.1 if torch.cuda.is_available() else 0}
         logger.debug(f"Creating actor: {actor_options}, {model_cls}")
         actor_cls = ray.remote(**actor_options)(model_cls)
         return actor_cls.remote(*spec.signature.init_args, **spec.signature.init_kwargs)
 
     def kill(self) -> None:
         """Kill the actor handle."""
         for actor_handle in self._actors:
```

## nos/models/__init__.py

```diff
@@ -1,8 +1,9 @@
 from nos import hub
 
+from ._noop import NoOp  # noqa: F401
 from .clip import CLIP  # noqa: F401
 from .faster_rcnn import FasterRCNN  # noqa: F401
 from .openmmlab.mmdetection.mmdetection import MMDetection  # noqa: F401
 from .sam import SAM
 from .stable_diffusion import StableDiffusion  # noqa: F401
 from .yolox import YOLOX  # noqa: F401
```

## nos/models/faster_rcnn.py

```diff
@@ -4,14 +4,15 @@
 import numpy as np
 import torch
 import torchvision.transforms.functional as F
 from PIL import Image
 
 from nos import hub
 from nos.common import ImageSpec, TaskType, TensorSpec
+from nos.common.io import prepare_images
 from nos.common.types import Batch, ImageT, TensorT
 from nos.hub import TorchHubConfig
 
 
 @dataclass(frozen=True)
 class FasterRCNNConfig(TorchHubConfig):
     pass
@@ -39,27 +40,16 @@
         self.model = fasterrcnn_mobilenet_v3_large_320_fpn(weights="DEFAULT").to(self.device)
         self.model.eval()
 
     def __call__(
         self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]
     ) -> Dict[str, np.ndarray]:
         """Predict bounding boxes for images."""
+        images = prepare_images(images)
         with torch.inference_mode():
-            if isinstance(images, np.ndarray):
-                images = [images]
-            elif isinstance(images, Image.Image):
-                images = [np.asarray(images)]
-            elif isinstance(images, list):
-                if isinstance(images[0], Image.Image):
-                    images = [np.asarray(image) for image in images]
-                elif isinstance(images[0], np.ndarray):
-                    pass
-                else:
-                    raise ValueError(f"Invalid type for images: {type(images[0])}")
-
             images = torch.stack([F.to_tensor(image) for image in images])
             images = images.to(self.device)
             predictions = self.model(images)
             return {
                 "bboxes": [pred["boxes"].cpu().numpy() for pred in predictions],
                 "scores": [pred["scores"].cpu().numpy() for pred in predictions],
                 "labels": [pred["labels"].cpu().numpy().astype(np.int32) for pred in predictions],
```

## nos/server/__init__.py

```diff
@@ -1,149 +1,231 @@
+import logging
+import math
+import subprocess
+from collections import deque
+from typing import List, Optional, Union
+
+import psutil
+import rich.status
+
 import docker
+import docker.errors
+import docker.models.containers
+from nos import __version__
 from nos.constants import DEFAULT_GRPC_PORT
+from nos.logging import logger
+
+from ._docker import DockerRuntime
+from ._runtime import InferenceServiceRuntime
+
+
+__all__ = ["init", "shutdown"]
 
 
 def init(
     runtime: str = "auto",
     port: int = DEFAULT_GRPC_PORT,
     utilization: float = 0.8,
     pull: bool = True,
+    logging_level: Union[int, str] = logging.INFO,
+    tag: Optional[str] = None,
 ) -> docker.models.containers.Container:
     """Initialize the inference server.
 
     Args:
         runtime (str, optional): The runtime to use (i.e. "cpu", "gpu"). Defaults to "auto".
             In "auto" mode, the runtime will be automatically detected.
         port (int, optional): The port to use for the inference server. Defaults to DEFAULT_GRPC_PORT.
         utilization (float, optional): The target cpu/memory utilization of inference server. Defaults to 0.8.
         pull (bool, optional): Pull the docker image before starting the inference server. Defaults to True.
+        logging_level (Union[int, str], optional): The logging level to use. Defaults to logging.INFO.
+            Optionally, a string can be passed (i.e. "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL").
+        tag (str, optional): The tag of the docker image to use ("latest"). Defaults to None, where the
+            appropriate version is used.
     """
-    import math
+    from nos.common.system import has_docker, has_gpu, has_nvidia_docker_runtime_enabled
 
-    import psutil
+    # Check arguments
+    available_runtimes = list(InferenceServiceRuntime.configs.keys()) + ["auto"]
+    if runtime not in available_runtimes:
+        raise ValueError(f"Invalid inference service runtime: {runtime}, available: {available_runtimes}")
+
+    if utilization <= 0.25 or utilization > 1:
+        raise ValueError(f"Invalid utilization: {utilization}, must be in (0.25, 1].")
+
+    if not isinstance(logging_level, (int, str)):
+        raise ValueError(f"Invalid logging level: {logging_level}, must be an integer or string.")
+    if isinstance(logging_level, int):
+        logging_level = logging.getLevelName(logging_level)
+    if logging_level not in ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
+        raise ValueError(f"Invalid logging level: {logging_level}")
 
-    from nos.common.system import get_system_info, has_gpu
-    from nos.logging import logger
-    from nos.server.runtime import InferenceServiceRuntime
+    if tag is None:
+        tag = __version__
+    else:
+        if not isinstance(tag, str):
+            raise ValueError(f"Invalid tag: {tag}, must be a string.")
+        raise NotImplementedError("Custom tags are not yet supported.")
 
     _MIN_NUM_CPUS = 4
     _MIN_MEM_GB = 6
     _MIN_SHMEM_GB = 4
 
+    # Determine runtime from system
+    if runtime == "auto":
+        runtime = "gpu" if has_gpu() else "cpu"
+        logger.debug(f"Detected system runtime: {runtime}")
+    else:
+        if runtime not in InferenceServiceRuntime.configs:
+            raise ValueError(
+                f"Invalid inference service runtime: {runtime}, available: {list(InferenceServiceRuntime.configs.keys())}"
+            )
+
     def _check_system_requirements():
-        # For now, we require at least 4 physical CPU cores and 6 GB of free memory
-        sysinfo = get_system_info()
-        num_cpus = sysinfo["cpu"]["cores"]["physical"]
-        mem_gb = sysinfo["memory"]["available"] / 1024**3
+        """Check system requirements."""
+        logger.debug(f"Checking system requirements, [nos={__version__}].")
+        if not has_docker():
+            raise RuntimeError("Docker not found, please install docker before proceeding.")
+
+        if runtime == "gpu":
+            if not has_gpu():
+                raise RuntimeError("GPU not found, please install CUDA drivers before proceeding.")
+            if not has_nvidia_docker_runtime_enabled():
+                raise RuntimeError(
+                    "NVIDIA Docker runtime not enabled, please enable NVIDIA Docker runtime before proceeding."
+                )
 
-        if num_cpus <= _MIN_NUM_CPUS:
+        # For now, we require at least 4 physical CPU cores and 6 GB of free memory
+        cl = DockerRuntime.get()._client
+        num_cpus = cl.info().get("NCPU", psutil.cpu_count(logical=False))
+        mem_avail = (
+            min(cl.info().get("MemTotal", psutil.virtual_memory().total), psutil.virtual_memory().available)
+            / 1024**3
+        )
+        logger.debug(f"Checking system requirements: [num_cpus={num_cpus}, mem_avail={mem_avail:.1f}GB]")
+        if num_cpus < _MIN_NUM_CPUS:
             raise ValueError(
                 f"Insufficient number of physical CPU cores ({num_cpus} cores), at least 4 cores required."
             )
-        if mem_gb <= _MIN_MEM_GB:
+        if mem_avail < _MIN_MEM_GB:
             raise ValueError(
-                f"Insufficient available system memory ({mem_gb:.1}GB), at least 6 GB of free memory required."
+                f"Insufficient available system memory ({mem_avail:.1f}GB), at least 6 GB of free memory required."
             )
 
     # Check system requirements
     _check_system_requirements()
 
-    # Check if inference server is already running
+    # Check if the latest inference server is already running
+    # If the running container's tag is inconsistent with the current version,
+    # we will shutdown the running container and start a new one.
     containers = InferenceServiceRuntime.list()
-    if len(containers) > 0:
-        assert (
-            len(containers) == 1
-        ), "Multiple inference servers running, please manually stop all containers before proceeding."
-        (container,) = containers
-        logger.warning(f"Inference server already running (name={container.name}, id={container.id[:12]}).")
-        return container
-
-    # Determine runtime from system
-    if runtime == "auto":
-        runtime = "gpu" if has_gpu() else "cpu"
-    else:
-        if runtime not in InferenceServiceRuntime.configs:
-            raise ValueError(
-                f"Invalid inference service runtime: {runtime}, available: {list(InferenceServiceRuntime.configs.keys())}"
+    if len(containers) == 1:
+        logger.debug("Found an existing inference server running, checking if it is the latest version.")
+        if InferenceServiceRuntime.configs[runtime].image not in containers[0].image.tags:
+            logger.info(
+                "Active inference server is not the latest version, shutting down before starting the latest one."
+            )
+            _stop_container(containers[0])
+        else:
+            (container,) = containers
+            logger.info(
+                f"Inference server already running (name={container.name}, image={container.image}, id={container.id[:12]})."
             )
+            return container
+    elif len(containers) > 1:
+        logger.warning("""Multiple inference servers running, please report this issue to the NOS maintainers.""")
+        for container in containers:
+            _stop_container(container)
+    else:
+        logger.debug("No existing inference server found, starting a new one.")
 
     # Pull docker image (if necessary)
     if pull:
         _pull_image(InferenceServiceRuntime.configs[runtime].image)
 
     # Start inference server
     runtime = InferenceServiceRuntime(runtime=runtime)
     logger.info(f"Starting inference service: [name={runtime.cfg.name}, runtime={runtime}]")
 
     # Determine number of cpus, system memory before starting container
-    num_cpus = max(_MIN_NUM_CPUS, utilization * psutil.cpu_count(logical=False))
-    mem_limit = max(_MIN_MEM_GB, utilization * math.floor(psutil.virtual_memory().available / 1024**3))
+    # Note (spillai): MacOSX compatibility issue where docker does not have access to
+    # the correct number of physical cores and memory.
+    cl = DockerRuntime.get()._client
+    num_cpus = cl.info().get("NCPU", psutil.cpu_count(logical=False))
+    num_cpus = max(_MIN_NUM_CPUS, utilization * num_cpus)
+    mem_limit = (
+        min(cl.info().get("MemTotal", psutil.virtual_memory().total), psutil.virtual_memory().available) / 1024**3
+    )
+    mem_limit = max(_MIN_MEM_GB, utilization * math.floor(mem_limit))
     logger.debug(f"Starting inference container: [num_cpus={num_cpus}, mem_limit={mem_limit}g]")
 
     # Start container
     container = runtime.start(
         nano_cpus=int(num_cpus * 1e9),
         mem_limit=f"{mem_limit}g",
         shm_size=f"{_MIN_SHMEM_GB}g",
         ports={f"{DEFAULT_GRPC_PORT}/tcp": port},
+        environment={
+            "NOS_LOGGING_LEVEL": logging_level,
+        },
+    )
+    logger.info(
+        f"Inference service started: [name={runtime.cfg.name}, runtime={runtime}, image={container.image}, id={container.id[:12]}]"
     )
-    logger.info(f"Inference service started: [name={runtime.cfg.name}, runtime={runtime}, id={container.id[:12]}]")
     return container
 
 
-def shutdown() -> docker.models.containers.Container:
+def shutdown() -> Optional[Union[docker.models.containers.Container, List[docker.models.containers.Container]]]:
     """Shutdown the inference server."""
-    from nos.logging import logger
-    from nos.server.runtime import InferenceServiceRuntime
-
     # Check if inference server is already running
     containers = InferenceServiceRuntime.list()
-    if not len(containers):
-        raise RuntimeError("Inference server not running, nothing to shutdown.")
+    if len(containers) == 1:
+        (container,) = containers
+        _stop_container(container)
+        return container
     if len(containers) > 1:
-        raise RuntimeError("Multiple inference servers running, please manually stop all containers.")
-    # Shutdown inference server
-    (container,) = containers
-    logger.info(f"Stopping inference service: [name={container.name}, id={container.id[:12]}]")
+        logger.warning("""Multiple inference servers running, please report this issue to the NOS maintainers.""")
+        for container in containers:
+            _stop_container(container)
+        return containers
+    logger.info("No active inference servers found, ignoring shutdown.")
+    return None
+
+
+def _stop_container(container: docker.models.containers.Container) -> None:
+    """Force stop containers."""
+    logger.info(
+        f"Stopping inference service: [name={container.name}, image={container.image}, id={container.id[:12]}]"
+    )
     try:
         container.remove(force=True)
     except Exception as e:
         raise RuntimeError(f"Failed to shutdown inference server: {e}")
-    logger.info(f"Inference service stopped: [name={container.name}, id={container.id[:12]}]")
-    return container
+    logger.info(f"Inference service stopped: [name={container.name}, image={container.image}, id={container.id[:12]}]")
 
 
 def _pull_image(image: str, quiet: bool = False, platform: str = None) -> str:
     """Pull the latest inference server image."""
-    import subprocess
-    from collections import deque
-
-    import rich.status
-
-    import docker.errors
-    from nos.logging import logger
-    from nos.server.runtime import DockerRuntime
-
     try:
         DockerRuntime.get()._client.images.get(image)
         logger.info(f"Found up-to-date server image: {image}")
     except docker.errors.ImageNotFound:
         try:
             logger.info(f"Pulling new server image: {image} (this may take a while).")
             # use subprocess to pull image and stream output
             proc = subprocess.Popen(
                 f"docker pull {image}", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True
             )
             status_q = deque(maxlen=25)
-            status_str = "Pulling new server image: {image} (this may take a while)."
-            with rich.status.Status(status_str) as status:
+            status_str = f"Pulling new server image: {image} (this may take a while)."
+            with rich.status.Status("[bold white]" + status_str + "[/bold white]") as status:
                 while proc.stdout.readable():
                     line = proc.stdout.readline()
                     if not line:
                         break
                     status_q.append(line.decode("utf-8").strip())
-                    status.update("[bold yellow]" + f"{status_str}\n\t" + "\n\t".join(status_q) + "[/bold yellow]")
+                    status.update("[bold white]" + f"{status_str}\n\t" + "\n\t".join(status_q) + "[/bold white]")
             proc.wait()
             logger.info(f"Pulled new server image: {image}")
         except (docker.errors.APIError, docker.errors.DockerException) as exc:
             logger.error(f"Failed to pull image: {image}, exiting early: {exc}")
             raise Exception(f"Failed to pull image: {image}, please mnaully pull image via `docker pull {image}`")
```

## nos/test/conftest.py

```diff
@@ -30,15 +30,15 @@
 
 
 @pytest.fixture(scope="session")
 def grpc_server(ray_executor):
     """Test gRPC server (Port: 50052)."""
     from loguru import logger
 
-    from nos.server.service import InferenceServiceImpl
+    from nos.server._service import InferenceServiceImpl
 
     logger.info(f"Starting gRPC test server on port: {GRPC_TEST_PORT}")
     options = [
         ("grpc.max_message_length", 512 * 1024 * 1024),
         ("grpc.max_send_message_length", 512 * 1024 * 1024),
         ("grpc.max_receive_message_length", 512 * 1024 * 1024),
     ]
@@ -73,15 +73,15 @@
 
     yield InferenceClient(f"[::]:{GRPC_TEST_PORT_GPU}")
 
 
 @pytest.fixture(scope="session")
 def grpc_server_docker_runtime_cpu():
     """Test DockerRuntime CPU (Port: 50053)."""
-    from nos.server.runtime import InferenceServiceRuntime
+    from nos.server import InferenceServiceRuntime
 
     CPU_CONTAINER_NAME = "nos-inference-service-runtime-cpu-e2e-test"
     runtime = InferenceServiceRuntime(runtime="cpu", name=CPU_CONTAINER_NAME)
 
     # Force stop any existing containers
     try:
         runtime.stop()
@@ -109,15 +109,15 @@
     except Exception:
         logger.info(f"Failed to stop existing container with name: {CPU_CONTAINER_NAME}")
 
 
 @pytest.fixture(scope="session")
 def grpc_server_docker_runtime_gpu():
     """Test DockerRuntime GPU (Port: 50054)."""
-    from nos.server.runtime import InferenceServiceRuntime
+    from nos.server import InferenceServiceRuntime
 
     GPU_CONTAINER_NAME = "nos-inference-service-runtime-gpu-e2e-test"
     runtime = InferenceServiceRuntime(runtime="gpu", name=GPU_CONTAINER_NAME)
 
     # Force stop any existing containers
     try:
         runtime.stop()
```

## Comparing `nos/server/docker.py` & `nos/server/_docker.py`

 * *Files 6% similar despite different names*

```diff
@@ -122,32 +122,32 @@
             container = self._client.containers.run(
                 image,
                 command=command,
                 name=name,
                 device_requests=device_requests,
                 **kwargs,
             )
-            logger.info(f"Started container: {name}")
-            logger.info(f"Get logs using `docker logs -f {container.id[:12]}`")
+            logger.debug(f"Started container [name={name}, image={container.image}, id={container.id[:12]}]")
+            logger.debug(f"Get logs using `docker logs -f {container.id[:12]}`")
         except (docker.errors.APIError, docker.errors.DockerException) as exc:
             logger.error(f"Failed to start container, cleaning up container: {exc}")
             self.stop(name)
             raise exc
         return container
 
     def stop(self, name: str, timeout: int = 30) -> docker.models.containers.Container:
         """Stop docker container."""
         try:
             container = self.get_container(name)
             if container is None:
                 logger.debug(f"Container not running: {name}, exiting early.")
                 return
-            logger.debug(f"Removing container: {name}")
+            logger.debug(f"Removing container: [name={name}, image={container.image}, id={container.id[:12]}]")
             container.remove(force=True)
-            logger.debug(f"Removed container: {name}")
+            logger.debug(f"Removed container: [name={name}, image={container.image}, id={container.id[:12]}]")
         except (docker.errors.APIError, docker.errors.DockerException) as exc:
             logger.error(f"Failed to stop container: {exc}")
         return container
 
     def get_container_id(self, name: str) -> Optional[str]:
         """Get the runtime container ID."""
         container = self.get_container(name)
```

## Comparing `nos/server/runtime.py` & `nos/server/_runtime.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """gRPC server runtime using docker executor."""
 import copy
 from dataclasses import dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Union
 
 import docker
-from nos.constants import DEFAULT_GRPC_PORT  # noqa F401
+from nos.constants import DEFAULT_GRPC_PORT, NOS_PROFILING_ENABLED  # noqa F401
 from nos.logging import LOGGING_LEVEL, logger
 from nos.protoc import import_module
-from nos.server.docker import DockerRuntime
 from nos.version import __version__
 
+from ._docker import DockerRuntime
+
 
 nos_service_pb2 = import_module("nos_service_pb2")
 nos_service_pb2_grpc = import_module("nos_service_pb2_grpc")
 
 
 NOS_DOCKER_IMAGE_CPU = f"autonomi/nos:{__version__}-cpu"
 NOS_DOCKER_IMAGE_GPU = f"autonomi/nos:{__version__}-gpu"
@@ -36,15 +37,20 @@
 
     command: Union[str, List[str]] = field(default_factory=lambda: [NOS_INFERENCE_SERVICE_CMD])
     """Command to run."""
 
     ports: Dict[int, int] = field(default_factory=lambda: {DEFAULT_GRPC_PORT: DEFAULT_GRPC_PORT})
     """Ports to expose."""
 
-    environment: Dict[str, str] = field(default_factory=lambda: {"NOS_LOGGING_LEVEL": LOGGING_LEVEL})
+    environment: Dict[str, str] = field(
+        default_factory=lambda: {
+            "NOS_LOGGING_LEVEL": LOGGING_LEVEL,
+            "NOS_PROFILING_ENABLED": int(NOS_PROFILING_ENABLED),
+        }
+    )
     """Environment variables."""
 
     volumes: Dict[str, Dict[str, str]] = field(
         default_factory=lambda: {
             str(Path.home() / ".nosd"): {"bind": "/app/.nos", "mode": "rw"},
         }
     )
@@ -139,15 +145,15 @@
 
     def start(self, **kwargs) -> docker.models.containers.Container:
         """Start the inference runtime.
 
         Args:
             **kwargs: Additional keyword-arguments to pass to `DockerRuntime.start`.
         """
-        logger.info(f"Starting inference runtime with image: {self.cfg.image}")
+        logger.debug(f"Starting inference runtime with image: {self.cfg.image}")
 
         # Override config with supplied kwargs
         for k in list(kwargs.keys()):
             value = kwargs[k]
             if hasattr(self.cfg, k):
                 setattr(self.cfg, k, value)
             else:
@@ -162,15 +168,15 @@
             ports=self.cfg.ports,
             environment=self.cfg.environment,
             volumes=self.cfg.volumes,
             detach=self.cfg.detach,
             gpu=self.cfg.gpu,
             **self.cfg.kwargs,
         )
-        logger.info(f"Started inference runtime: {self}")
+        logger.debug(f"Started inference runtime: {self}")
         return container
 
     def stop(self, timeout: int = 30) -> docker.models.containers.Container:
         return self._runtime.stop(self.cfg.name, timeout=timeout)
 
     def get_container(self) -> docker.models.containers.Container:
         return self._runtime.get_container(self.cfg.name)
```

## Comparing `nos/server/service.py` & `nos/server/_service.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,30 +1,40 @@
+import time
 import traceback
+from functools import lru_cache
 from typing import Any, Dict
 
 import grpc
 import rich.console
 import rich.status
 from google.protobuf import empty_pb2
 
 from nos import hub
 from nos.common import ModelSpec, TaskType, dumps
-from nos.constants import DEFAULT_GRPC_PORT  # noqa F401
+from nos.constants import DEFAULT_GRPC_PORT, NOS_PROFILING_ENABLED  # noqa F401
 from nos.exceptions import ModelNotFoundError
 from nos.executors.ray import RayExecutor
 from nos.logging import logger
 from nos.managers import ModelHandle, ModelManager
 from nos.protoc import import_module
 from nos.version import __version__
 
 
 nos_service_pb2 = import_module("nos_service_pb2")
 nos_service_pb2_grpc = import_module("nos_service_pb2_grpc")
 
 
+@lru_cache(maxsize=32)
+def load_spec(model_name: str, task: TaskType) -> ModelSpec:
+    """Get the model spec cache."""
+    model_spec: ModelSpec = hub.load_spec(model_name, task=task)
+    logger.info(f"Loaded model spec: {model_spec}")
+    return model_spec
+
+
 class InferenceService:
     """Ray-executor based inference service.
 
     Parameters:
         model_manager (ModelManager): Model manager.
         executor (RayExecutor): Ray executor.
 
@@ -35,51 +45,51 @@
         self.model_manager = ModelManager()
         self.executor = RayExecutor.get()
         try:
             self.executor.init()
         except Exception as e:
             logger.info(f"Failed to initialize executor: {e}")
             raise RuntimeError(f"Failed to initialize executor: {e}")
-        self.current_model_id = None
-        self.current_model_spec = None
 
     def execute(self, model_name: str, task: TaskType = None, inputs: Dict[str, Any] = None) -> Dict[str, Any]:
         """Execute the model.
 
         Args:
             model_name (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).
             task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).
             inputs (Dict[str, Any]): Model inputs.
         Returns:
             Dict[str, Any]: Model outputs.
         """
-        # Load model spec if the model has changed
-        if self.current_model_id is None or self.current_model_id != (model_name, task.value):
-            # Load the model spec
-            try:
-                model_spec: ModelSpec = hub.load_spec(model_name, task=task)
-                logger.debug(f"Loaded model spec: {model_spec}")
-            except Exception as e:
-                raise ModelNotFoundError(f"Failed to load model spec: {model_name}, {e}")
-            self.current_model_id = (model_name, task.value)
-            self.current_model_spec = model_spec
-
-        assert self.current_model_spec is not None
-        model_spec = self.current_model_spec
+        # Load the model spec
+        try:
+            model_spec: ModelSpec = load_spec(model_name, task=task)
+        except Exception as e:
+            raise ModelNotFoundError(f"Failed to load model spec: {model_name}, {e}")
 
         # TODO (spillai): Validate/Decode the inputs
+        mid = time.perf_counter()
         model_inputs = model_spec.signature._decode_inputs(inputs)
+        model_inputs_types = [
+            f"{k}: List[type={type(v[0])}, len={len(v)}]" if isinstance(v, list) else str(type(v))
+            for k, v in model_inputs.items()
+        ]
+        logger.debug(
+            f"Decoded inputs [inputs=({', '.join(model_inputs_types)}), elapsed={(time.perf_counter() - mid) * 1e3:.1f}ms]"
+        )
 
         # Initialize the model (if not already initialized)
         # This call should also evict models and garbage collect if
         # too many models are loaded are loaded simultaneously.
         model_handle: ModelHandle = self.model_manager.get(model_spec)
 
         # Get the model handle and call it remotely (with model spec, actor handle)
+        mid = time.perf_counter()
         response: Dict[str, Any] = model_handle.remote(**model_inputs)
+        logger.debug(f"Executed model [name={model_spec.name}, elapsed={(time.perf_counter() - mid) * 1e3:.1f}ms]")
 
         # If the response is a single value, wrap it in a dict with the appropriate key
         if len(model_spec.signature.outputs) == 1:
             response = {k: response for k in model_spec.signature.outputs}
 
         return response
 
@@ -117,14 +127,15 @@
     def GetModelInfo(
         self, request: nos_service_pb2.ModelInfoRequest, context: grpc.ServicerContext
     ) -> nos_service_pb2.ModelInfoResponse:
         """Get model information."""
         try:
             model_info = request.request
             spec: ModelSpec = hub.load_spec(model_info.name, task=TaskType(model_info.task))
+            logger.debug(f"GetModelInfo(): {spec}")
         except KeyError as e:
             logger.error(f"Failed to load spec: [request={request.request}, e={e}]")
             context.abort(grpc.StatusCode.NOT_FOUND, str(e))
         return spec._to_proto(public=True)
 
     def Run(
         self, request: nos_service_pb2.InferenceRequest, context: grpc.ServicerContext
@@ -134,23 +145,28 @@
         logger.debug(f"Received request: {model_request.task}, {model_request.name}")
         if model_request.task not in (
             TaskType.IMAGE_GENERATION.value,
             TaskType.IMAGE_EMBEDDING.value,
             TaskType.TEXT_EMBEDDING.value,
             TaskType.OBJECT_DETECTION_2D.value,
             TaskType.IMAGE_SEGMENTATION_2D.value,
+            TaskType.CUSTOM.value,
         ):
             context.abort(grpc.StatusCode.NOT_FOUND, f"Invalid task {model_request.task}")
 
         try:
-            logger.debug(f"Executing request: {model_request.task}, {model_request.name}")
+            st = time.perf_counter()
+            logger.info(f"Executing request [task={model_request.task}, name={model_request.name}]")
             response = self.execute(model_request.name, task=TaskType(model_request.task), inputs=request.inputs)
+            logger.info(
+                f"Executed request [task={model_request.task}, model={model_request.name}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]"
+            )
             return nos_service_pb2.InferenceResponse(response_bytes=dumps(response))
         except (grpc.RpcError, Exception) as e:
-            msg = f"Failed to execute request: {model_request.task}, {model_request.name}"
+            msg = f"Failed to execute request: [task={model_request.task}, model={model_request.name}]"
             msg += f"{traceback.format_exc()}"
             logger.error(f"{msg}, e={e}")
             context.abort(grpc.StatusCode.INTERNAL, "Internal Server Error")
 
 
 def serve(address: str = f"[::]:{DEFAULT_GRPC_PORT}", max_workers: int = 1) -> None:
     """Start the gRPC server."""
```

## Comparing `autonomi_nos-0.0.6a1.dist-info/LICENSE` & `autonomi_nos-0.0.7a2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autonomi_nos-0.0.6a1.dist-info/METADATA` & `autonomi_nos-0.0.7a2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: autonomi-nos
-Version: 0.0.6a1
+Version: 0.0.7a2
 Summary: Nitrous oxide system (NOS) for computer-vision.
 License: MIT License
         
         Copyright (c) 2023 Autonomi AI
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
         of this software and associated documentation files (the "Software"), to deal
@@ -36,22 +36,22 @@
 Classifier: Programming Language :: Python :: 3.8
 Requires-Python: >=3.7.10
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: av (>=10.0.0)
 Requires-Dist: cloudpickle (>=2.2.1)
 Requires-Dist: docker (>=6.0.0)
-Requires-Dist: grpcio-tools (<=1.49.1,>=1.32.0)
+Requires-Dist: grpcio-tools (<=1.49.1)
 Requires-Dist: loguru (>=0.7.0)
 Requires-Dist: opencv-python-headless (>=4.6.0.66)
 Requires-Dist: pandas
 Requires-Dist: Pillow
 Requires-Dist: psutil (>=5.9.5)
 Requires-Dist: py-cpuinfo (>=9.0.0)
-Requires-Dist: pydantic
+Requires-Dist: pydantic (<2)
 Requires-Dist: rich (>=12.5.1)
 Requires-Dist: tqdm
 Requires-Dist: typer (>=0.7.0)
 Requires-Dist: typing-extensions (>=4.5.0)
 Provides-Extra: dev
 Requires-Dist: black[jupyter] (==22.3.0) ; extra == 'dev'
 Requires-Dist: build (==0.10.0) ; extra == 'dev'
```

## Comparing `autonomi_nos-0.0.6a1.dist-info/RECORD` & `autonomi_nos-0.0.7a2.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,64 +1,66 @@
 nos/__init__.py,sha256=hZuUzrXY9Px7KEt6deoV_-NKYiXLF_jtlQVA5hIPV0s,164
-nos/constants.py,sha256=j-nz48q31rE7eclMqkZH6xFJ0eGmtx2Jt5yr_wQA_1Y,561
+nos/constants.py,sha256=5uSx6pq0sLVVU-mBAuagfq-gEzpTRwSxrDV0aALW7qk,635
 nos/exceptions.py,sha256=ocRCxTbzT_Q8WTobAbkbo4jl5a6F3xC_D2N7rtzfWeg,93
 nos/logging.py,sha256=D0QDNFGmAYshfnSC-ctEDSIeBO6TnLPz1babUmJGRSA,940
 nos/protoc.py,sha256=xUck1U30UqUEnTaFuWMGWagyDcIrjmcQo8xozqbypmI,2778
-nos/version.py,sha256=gXd1D56xj6WOvmCczvF5j2U21R-6e_8jAsOhKTHAwGk,24
-nos/cli/cli.py,sha256=59Q5GUat7rQsNZco1JX27l1W6KCYfri8ctAGvOuFzec,369
-nos/cli/docker.py,sha256=SDGg8b343r6ndzmyESGbHLQx7EjGwkV2Plo025Y-U5A,4230
+nos/version.py,sha256=wE0kuii8zMPGmvMf1N__Nkj1SPLydCz9UHFwb-9SV-s,24
+nos/cli/benchmark.py,sha256=k3ii0IyJ13rwziTqMUv9BxRP-iNX5p8bKb7tRJQqkcc,11371
+nos/cli/cli.py,sha256=DvLBCstXKrUY2DXeBKCfMxeN7sLd433AzPncyrbAxXo,540
+nos/cli/docker.py,sha256=VCAxJz33JKBsL7-Wq3Ui8creGiahR_chX0o5F8caOHs,4222
 nos/cli/hub.py,sha256=USdzVgaZlPZABrrIZzCA7ySwXg1iseEk7GJ7iwanXIg,1294
 nos/cli/predict.py,sha256=eo9Fc2ZmzaA6z1QenacKIYHMX4DYwCnIQHf3XTpyMho,7543
 nos/cli/serve_http.py,sha256=xMwSE46-_uHLgif1gnJCRoLbBE3ttC2zkm2JXVooWg0,5962
-nos/cli/system.py,sha256=MyVBcPZDlpiAJHhdK4JiFKyTJfD1iez3JzLvG6lsMAw,1713
+nos/cli/system.py,sha256=wu4xrbQiVx7K1sTg07n5qeGrJ8q2NS_28k1a1J1sy4E,1706
 nos/cli/utils.py,sha256=uR1lGZyyDEuflNvxKuAmVUP-DTE55PzZL5c0c3l2h4U,425
 nos/client/__init__.py,sha256=oXG9pYr-XdtjExlDenSsleOzeCBzLGYIWwzD9-rkm6o,396
-nos/client/exceptions.py,sha256=Lqqeu_6oNSZgOUipXHFx20yeASJi9bEewpG1UYuy1gk,92
-nos/client/grpc.py,sha256=T0Rq3YEy9RQ8t-6Kjsbyn5vd1MxoxowKLNhiuqB4RBQ,12378
+nos/client/exceptions.py,sha256=76qw6r-5VYGm2HOXI6STgSo69dN7NpDHCWQeNvtedsw,351
+nos/client/grpc.py,sha256=BD4dHtohSIYm2H_lNWxn2b1CuSlQlnFnBQlhj4NMpuU,13192
 nos/common/__init__.py,sha256=I4hKyXjU0xs80tbzE4sjDLAiOCFUci67xBpYMkhsa4Y,1285
 nos/common/cloudpickle.py,sha256=2VBtGaLHbVtKg9ICT-xUITweHQCd6PxsFobDKwSYE2I,204
-nos/common/profiler.py,sha256=8Q82nyNd81xHAQAj1MMdc1t9tfdTygTHmiNLPMdY-KY,2712
+nos/common/profiler.py,sha256=WrTJJZu3EeT630VfBwbHbz0vi3tDh4r0CakTe-kW8Jo,10364
 nos/common/spec.py,sha256=_mcwc_OhflG19OwT5SkOtTsHDRuiceJKixOqf-HKI2g,9446
 nos/common/system.py,sha256=OpcOu91GsNbESiY6KADz4RRn9uk4xKLF0AtYbG3MvME,6076
-nos/common/tasks.py,sha256=AcEbh5gbpKt_77Ar9zRrRyiD8tyw264Wv2buyX9yxZ8,518
+nos/common/tasks.py,sha256=4tpOPldlesiZTooo6RIZInhvxj818mp8h0Wyu_oMp20,593
 nos/common/types.py,sha256=MKNH-7qW8N1L-7nYH7b7NIgoSkt2JepcWxoLUvzX6QM,5984
-nos/common/io/__init__.py,sha256=r8y2TSd0Zzv7bsImJCSjP7S1RMckRFTPyGmKHQnxTgI,942
+nos/common/io/__init__.py,sha256=NW8jGIcTResNmL7LeAlir6aHbFs6FU_n6no-2bdm2i8,1499
 nos/common/io/video/base.py,sha256=rAEBnj-LqkeqlCAXqGbAP49zKL3Q-m3sHhtLo6vh2ak,1050
 nos/common/io/video/opencv.py,sha256=s5CXfsbbk0IAXA42VvAFuaBO-d0nqaClZ2Qw7xU8taA,8424
 nos/compilers/__init__.py,sha256=viIY6XbOWebZnYFwKsgKxx9GbE5SXkn8alAjJbFTp_g,5832
 nos/compilers/trt/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 nos/compilers/trt/ops/group_norm.py,sha256=suIAbosSrDPMyKk_uuEhRPCggK73_wbE8YGSIMWanvk,2739
 nos/executors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 nos/executors/ray.py,sha256=np0tmPRRRqVfO__ziKcYqPrZ_llLOSk_N8-BNXlzpOs,7544
-nos/hub/__init__.py,sha256=OOPhhVKINO8Jp-JZ6FmxsPfOW5k1ikauD-5k7Poi6Fk,3531
+nos/hub/__init__.py,sha256=NWR3VWRshyMjw6LMLkK_aVs-00m4Gte7Ltf2UeD905c,3615
 nos/hub/config.py,sha256=DK0t6xrB6Sgf71YWnX_htNbbU4H2Bcdu9b7ujiA6QKY,2195
 nos/managers/__init__.py,sha256=ZLKQSo3Wj5B2fFyUiTTQlgQ97KWcpxJZtO27DseVbsU,59
-nos/managers/model.py,sha256=k7E-5wW7iJiCUNLfyM2DG3kZJ36Af8UX4e85jdmZJLQ,7911
-nos/models/__init__.py,sha256=uoT6_uSo7nN9DZE-z8xl-ul5hC2RUvqHHufHi7_axQQ,301
+nos/managers/model.py,sha256=wNNCO90eQ2k5_0gDhqbE_UE8bnmJBp4cM3fi_c8qGzA,7911
+nos/models/__init__.py,sha256=7qIGd2FCQMyijmHRPgYW9uLG52hiBtGCOlTG0gT3C40,339
+nos/models/_noop.py,sha256=lH0FmIdG2xy-wvSZQP9BEEMjbRhAVtLUCa6RGUjZKCQ,879
 nos/models/clip.py,sha256=N2QB4gWVEHqfBKoSdvX2o2nX9_9TCE7MySGRIj49IQA,9931
-nos/models/faster_rcnn.py,sha256=KF-NYOYQbyUHvhgkQKekSFbTnfxYSBHbN1N_uLkOxRA,3285
+nos/models/faster_rcnn.py,sha256=7cy83dKT4Nfj6sEFA2Xzc3XMXzgYbIcgaKlqaBDR_yQ,2834
 nos/models/sam.py,sha256=bMGxM9unwmCA9TfHOI9IFwW5iLlOwxbO7OmcBCtLNJg,1507
 nos/models/stable_diffusion.py,sha256=oCXjjwlMXssLS3f7-voN-2Sj-84fyW62-KEu7U1ZgJc,16095
 nos/models/yolox.py,sha256=wuOkm6jCmNLgZyu2Uad2WnW_YKlqqqYx7R19_nBGpGs,10128
 nos/models/openmmlab/__init__.py,sha256=VzlwVzMv9EVqdyKwvTDBaVhH9RYnYv0zEU_v4wEdR4w,63
 nos/models/openmmlab/mmdetection/mmdetection.py,sha256=gjWBD3t4CXEFd5t-k_qBHbk_Hi588nn-YGll68jM4Eo,3066
 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py,sha256=tAbybUUg9TtBy_dqiXD5Zgl0kSs39JX7TiTeIYZrOUo,370
 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py,sha256=MxNi_Sf1m8CNkR64vZWaFN2O2P2LBI1-EexpLr166YU,3187
 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py,sha256=DVkj2zskGrXxiF6hVQ1ofsOsqQGHsdjAh5sXRkaxjiw,1765
 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py,sha256=Al2qpY2P-MzsfBCod5pjB6HrH7ZzFvQxICc1CVmpnRY,3828
 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py,sha256=G8gXisLhM7mBRlxrhTOLCsWD17zCyH7kBvVi7J4BICI,304
 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py,sha256=RSkd9o1IO-YkWhIwnXU83tRyGeySiR1IMlSk14dTw9s,5340
 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py,sha256=Cm8lyQ3wOT5I9KR0m1TfTsPAGybs-buILRPLDZKzOPQ,177
 nos/proto/nos_service.proto,sha256=BEWdRsehHLaSjH2K2QEhtmaDqkwDZYdxKm1t4yg_0V8,2437
-nos/server/__init__.py,sha256=5RiKswYy4smsuFOVba9Rq3beaeI5Mifwxdfi0mODH5M,6193
-nos/server/docker.py,sha256=XvubrQzj1XgYH-QDOWPG5SIBy3fhoexFn_mW6bKZk_M,6556
-nos/server/runtime.py,sha256=maiFx1bpygU17ZOdJr5T2wu5kRahM4kP2R1B2Uj7X04,6363
-nos/server/service.py,sha256=k2Xkv-uNnEm9AlMGcSHy-dL81uKQqyyf8O2u__ak3-4,7326
+nos/server/__init__.py,sha256=x6EzKKWVhc_FsqwtdAARS6uswVZCqUFemEv7wt1M7PU,10198
+nos/server/_docker.py,sha256=Hrq40bwFPejA2k0fX8MooTwsYbrAoped8Uk7KAOWwfU,6725
+nos/server/_runtime.py,sha256=ppYsFHoXqiIU0nJzFPlknXon1GO13WxCoY_idkgCfnw,6482
+nos/server/_service.py,sha256=aEqML7qaYJ5rcyf4KBStklwshg-7-Sra9G9vtx3EZJI,7989
 nos/test/benchmark.py,sha256=b_QMHfStY5iRjHGPZwaY7VrXzy_VeFKfjld9UEVbn-g,373
-nos/test/conftest.py,sha256=-NWElgDWfFZpYwmGk1WpSB9JCAtZz3gKx1DJhDivoPs,4252
+nos/test/conftest.py,sha256=20YuaBtHaMNUDEkvuHFl32J4jX5zpHGc-jTcvPHo4bk,4237
 nos/test/utils.py,sha256=bi-aoXsTmDiwmwlWVyhznW_ZS7k7N9bZJuq-QuMz2OA,2626
-autonomi_nos-0.0.6a1.dist-info/LICENSE,sha256=9TQFxQ2AkXOQuIHy9GueB_a18hayRXT7pDt9fJv9WLo,1068
-autonomi_nos-0.0.6a1.dist-info/METADATA,sha256=A1QLJKOL6vuRkxoahpJfWCNkhWVReb6qYaAF3zOTcgM,6804
-autonomi_nos-0.0.6a1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-autonomi_nos-0.0.6a1.dist-info/entry_points.txt,sha256=UYtJAmFT3RPWmlKM11MMZvZPRHTeKyIh1BbZ3QpbsJs,86
-autonomi_nos-0.0.6a1.dist-info/top_level.txt,sha256=Tgqk49XI1nXvi6W_Ryy7_YwQ7iFU-mAlIsbNMR1HS6s,4
-autonomi_nos-0.0.6a1.dist-info/RECORD,,
+autonomi_nos-0.0.7a2.dist-info/LICENSE,sha256=9TQFxQ2AkXOQuIHy9GueB_a18hayRXT7pDt9fJv9WLo,1068
+autonomi_nos-0.0.7a2.dist-info/METADATA,sha256=nSFaGXKg3oTB80yk2BO3U2_7yjP7JpdOk5nJOpp-HDo,6800
+autonomi_nos-0.0.7a2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+autonomi_nos-0.0.7a2.dist-info/entry_points.txt,sha256=KcZyneDX7Iv7PMPuohaLfe2xQMUVnE7S2apsS5aIshA,87
+autonomi_nos-0.0.7a2.dist-info/top_level.txt,sha256=RgOntmzdTkyrY-Fj9H6bSke7af3bHLscoZnK1-7Aa8o,12
+autonomi_nos-0.0.7a2.dist-info/RECORD,,
```

